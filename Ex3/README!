Q1
n mutex  for each map thread
1 conditional variable cv and mutex cv_mutex

produceFunc -- map thread
    lock mutex i
    add (k2, V2) to monitor[i]
    unlock mutex i
    notify cv


consumerFunc -- shuffle thread
    while(true)
        wait(cv, cv_mutex)
        unlock cv_mutex
        for i = 1 ... n
            lock mutex i
            shuffle the data in map i container
            unlock mutex i
         end for
    end while

The timed wait is needed because there is a scenario where while the shuffle thread
is running (not waiting), all the map threads will finish and notify the
shuffle thread. All notifications will be lost and the next time it will reach
the wait cv it will wait for ever if not timed.

Q2
multiThreadLevel = 7
to utilize the map reduce procedure we will need to use all cores.
when multiThreadLevel = 7, 7 map threads, 1 shuffle thread and the main thread
will run in parallel. in the map part of the procedure the main thread waits for
the shuffle to finish and it does not consume any cpu time. This way the 7 map threads and
the shuffle thread run continuously and no time is waisted on thread switching while all cores
are working all time.

Q3
a. Utilizing multi-cores
Nira - not utilizing at all.
Moti - good utilization kernel level threads run on different cores
Danny - the computer does not know there are more then one thread and no utilization
        is happening
Galit - good utilization processes run on different cores


b. The ability to create a sophisticated scheduler, based on internal data.
Nira - no scheduling at all
Moti - OS scheduling not able to modify
Danny - User level thread anable full control on scheduling
Galit - OS scheduling not able to modify

c. Communication time (between different threads/processes)
Nira - no communication needed. no time is waisted
Moti - slower than Danny. communication through shared data on different cores
Danny - fast communication. all data with in the same core
Galit - communication through signals slower than Moti.

d. Ability to progress while a certain thread/process is blocked (e.g. is waiting
for disk)
Nira - one process no progress
Moti - Other threads run in parallel time, they all make progress while one thread
       is waiting
Danny - one process no progress
Galit - Other processes run in parallel time, they all make progrees while one process
        is waiting

Assuming the frame work has multi-cores cpu like most machines nowadays.
e. Overall speed
Nira - saves time on switches and mutex waiting. Best in short tasks.
        Probably the slowest time of all implementation.
Moti - Saves time on waiting and utilize a big task by dividing between many threads.
        The fastest when minimizing the time on waiting for shared data
Danny - utilize the schedule time. Losses time on waiting time for disk or cpu.
        Faster than Nira
Galit - Similar to Moti, losses time on sharing data between processes.

Q4
Kernel level thread
Stack - not shared
Heap - shared, same process
Global variables - shared, same process

User level thread
Stack - not shared
Heap - shared, same process
Global variables - shared, same process

Process
Stack - not shared
Heap - not shared
Global variables - not shared

Q5
A deadlock is a scenario where two or more tasks are waiting for each other in a cycle.
A livelock is a similar to dead lock only the tasks are not in waiting mode. their mode
changes in way no progress is made.

Deadlock
thread 1
    lock mutex a
    lock mutex b
    ...code
    unlock mutex b
    unlock mutex a

thread 2
    lock mutex b
    lock mutex a
    ...code
    unlock mutex a
    unlock mutex b

thread 1 locks mutex a, thread 2 locks mutex b and they both locked forever

Livelock

Thread1
    while (true)
        if lock mutex a failed
            wait 1000
            continue

        if lock mutex b failed
             wait 1000
             continue

         ...code


Thread2
    while (true)
        if lock mutex b failed
            wait 1000
            continue

        if lock mutex a failed
             wait 1000
             continue

         ...code

the threads never reach the code